"""
LLM Integration Test Suite
Comprehensive testing for Trading Agent v1.5 with real Anthropic API

This test suite validates:
1. Basic LLM connectivity
2. Trading decision accuracy
3. Performance characteristics
4. Error handling
5. Integration with existing tools
"""

import asyncio
import json
import os
import time
from dataclasses import asdict, dataclass
from datetime import datetime
from typing import Any

# Import your LLM clients
from src.trading_agent.llm import AnthropicLLMClient


@dataclass
class TestResult:
    """Test result container"""
    test_name: str
    success: bool
    duration_ms: float
    details: dict[str, Any]
    error: str | None = None

@dataclass
class PerformanceMetrics:
    """Performance metrics for LLM calls"""
    avg_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    total_tokens: int
    avg_tokens_per_call: float
    error_rate: float

class LLMIntegrationTester:
    """Comprehensive test suite for LLM integration"""

    def __init__(self, api_key: str | None = None):
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY required for testing")

        self.client = AnthropicLLMClient(api_key=self.api_key)
        self.test_results: list[TestResult] = []
        self.performance_data: list[dict[str, Any]] = []

    def run_all_tests(self) -> dict[str, Any]:
        """Run complete test suite"""

        print("🧪 Starting LLM Integration Test Suite")
        print("=" * 50)

        # Basic connectivity tests
        self._test_basic_connectivity()
        self._test_basic_completion()

        # Trading-specific tests
        self._test_trading_decision()
        self._test_tool_integration()
        self._test_market_scenarios()

        # Performance tests
        self._test_performance_characteristics()
        self._test_concurrent_requests()

        # Error handling tests
        self._test_error_handling()
        self._test_fallback_scenarios()

        # Generate report
        return self._generate_test_report()

    def _test_basic_connectivity(self):
        """Test basic API connectivity"""

        start_time = time.time()

        try:
            response = self.client.complete("Hello Claude, please respond with exactly: 'Integration test successful'")

            duration_ms = (time.time() - start_time) * 1000

            success = "integration test successful" in response.content.lower()

            result = TestResult(
                test_name="basic_connectivity",
                success=success,
                duration_ms=duration_ms,
                details={
                    "response_length": len(response.content),
                    "tokens_used": response.tokens_used,
                    "model": response.model_used,
                    "confidence": response.confidence
                }
            )

            self.test_results.append(result)
            self._log_test_result(result)

        except Exception as e:
            result = TestResult(
                test_name="basic_connectivity",
                success=False,
                duration_ms=(time.time() - start_time) * 1000,
                details={},
                error=str(e)
            )
            self.test_results.append(result)
            self._log_test_result(result)

    def _test_basic_completion(self):
        """Test basic completion functionality"""

        start_time = time.time()

        try:
            prompt = """
            Analyze this trading scenario:
            - Symbol: EURUSD
            - Current price: 1.0950
            - RSI: 30 (oversold)
            - MACD: -0.0015 (bearish)
            - News: ECB considering rate cuts

            Should I buy, sell, or hold? Respond with just one word.
            """

            response = self.client.complete(prompt)
            duration_ms = (time.time() - start_time) * 1000

            # Check if response contains a trading action
            content_lower = response.content.lower()
            has_action = any(action in content_lower for action in ['buy', 'sell', 'hold'])

            result = TestResult(
                test_name="basic_completion",
                success=has_action and len(response.content) > 0,
                duration_ms=duration_ms,
                details={
                    "response": response.content[:100],
                    "contains_action": has_action,
                    "tokens_used": response.tokens_used
                }
            )

            self.test_results.append(result)
            self._log_test_result(result)

        except Exception as e:
            result = TestResult(
                test_name="basic_completion",
                success=False,
                duration_ms=(time.time() - start_time) * 1000,
                details={},
                error=str(e)
            )
            self.test_results.append(result)
            self._log_test_result(result)

    def _test_trading_decision(self):
        """Test structured trading decision making"""

        start_time = time.time()

        try:
            context = {
                "symbol": "EURUSD",
                "prices": [1.0900, 1.0905, 1.0910, 1.0915, 1.0920],
                "indicators": {
                    "RSI": 65.5,
                    "MACD": 0.0012,
                    "signal": "BULLISH"
                },
                "account_info": {
                    "balance": 10000.0,
                    "equity": 10000.0,
                    "free_margin": 9000.0
                }
            }

            tools = [
                {
                    "name": "calc_rsi",
                    "description": "Calculate RSI indicator",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "prices": {"type": "array"},
                            "period": {"type": "integer"}
                        }
                    }
                }
            ]

            decision = self.client.reason_with_tools(context, tools, "trading")
            duration_ms = (time.time() - start_time) * 1000

            # Validate decision structure
            required_fields = ["action", "confidence", "reasoning", "lots"]
            has_required_fields = all(field in decision for field in required_fields)

            valid_action = decision.get("action") in ["BUY", "SELL", "HOLD"]
            valid_confidence = 0.0 <= decision.get("confidence", -1) <= 1.0

            result = TestResult(
                test_name="trading_decision",
                success=has_required_fields and valid_action and valid_confidence,
                duration_ms=duration_ms,
                details={
                    "decision": decision,
                    "has_required_fields": has_required_fields,
                    "valid_action": valid_action,
                    "valid_confidence": valid_confidence
                }
            )

            self.test_results.append(result)
            self._log_test_result(result)

        except Exception as e:
            result = TestResult(
                test_name="trading_decision",
                success=False,
                duration_ms=(time.time() - start_time) * 1000,
                details={},
                error=str(e)
            )
            self.test_results.append(result)
            self._log_test_result(result)

    def _test_tool_integration(self):
        """Test tool calling functionality"""

        start_time = time.time()

        try:
            # Define a simple tool
            tools = [
                {
                    "name": "calculate_position_size",
                    "description": "Calculate position size based on risk",
                    "input_schema": {
                        "type": "object",
                        "properties": {
                            "account_balance": {"type": "number"},
                            "risk_percent": {"type": "number"},
                            "stop_loss_pips": {"type": "number"}
                        },
                        "required": ["account_balance", "risk_percent", "stop_loss_pips"]
                    }
                }
            ]

            prompt = "I have $10,000 account, want to risk 2%, stop loss 20 pips on EURUSD. Use the tool to calculate position size."

            response = self.client.complete(prompt, tools=tools)
            duration_ms = (time.time() - start_time) * 1000

            # Check if tools were mentioned or used
            tools_mentioned = "calculate_position_size" in response.content or "tool" in response.content.lower()

            result = TestResult(
                test_name="tool_integration",
                success=len(response.content) > 0 and tools_mentioned,
                duration_ms=duration_ms,
                details={
                    "response_snippet": response.content[:200],
                    "tools_mentioned": tools_mentioned,
                    "tokens_used": response.tokens_used
                }
            )

            self.test_results.append(result)
            self._log_test_result(result)

        except Exception as e:
            result = TestResult(
                test_name="tool_integration",
                success=False,
                duration_ms=(time.time() - start_time) * 1000,
                details={},
                error=str(e)
            )
            self.test_results.append(result)
            self._log_test_result(result)

    def _test_market_scenarios(self):
        """Test various market scenarios"""

        scenarios = [
            {
                "name": "trending_up",
                "context": {
                    "symbol": "GBPUSD",
                    "prices": [1.2500, 1.2510, 1.2520, 1.2530, 1.2540],
                    "indicators": {"RSI": 55, "MACD": 0.0020},
                    "expected_bias": "bullish"
                }
            },
            {
                "name": "trending_down",
                "context": {
                    "symbol": "USDJPY",
                    "prices": [150.00, 149.80, 149.60, 149.40, 149.20],
                    "indicators": {"RSI": 35, "MACD": -0.0030},
                    "expected_bias": "bearish"
                }
            },
            {
                "name": "sideways",
                "context": {
                    "symbol": "EURUSD",
                    "prices": [1.0900, 1.0905, 1.0900, 1.0895, 1.0900],
                    "indicators": {"RSI": 50, "MACD": 0.0001},
                    "expected_bias": "neutral"
                }
            }
        ]

        scenario_results = []

        for scenario in scenarios:
            start_time = time.time()

            try:
                decision = self.client.reason_with_tools(
                    scenario["context"],
                    [],
                    "trading"
                )

                duration_ms = (time.time() - start_time) * 1000

                # Analyze if decision aligns with expected bias
                action = decision.get("action", "HOLD")
                expected = scenario["expected_bias"]

                alignment = (
                    (expected == "bullish" and action == "BUY") or
                    (expected == "bearish" and action == "SELL") or
                    (expected == "neutral" and action == "HOLD")
                )

                scenario_result = {
                    "scenario": scenario["name"],
                    "success": alignment,
                    "action": action,
                    "expected": expected,
                    "confidence": decision.get("confidence", 0),
                    "duration_ms": duration_ms
                }

                scenario_results.append(scenario_result)

            except Exception as e:
                scenario_result = {
                    "scenario": scenario["name"],
                    "success": False,
                    "error": str(e),
                    "duration_ms": (time.time() - start_time) * 1000
                }
                scenario_results.append(scenario_result)

        # Overall result
        success_rate = sum(1 for r in scenario_results if r["success"]) / len(scenario_results)

        result = TestResult(
            test_name="market_scenarios",
            success=success_rate >= 0.5,  # At least 50% success rate
            duration_ms=sum(r["duration_ms"] for r in scenario_results),
            details={
                "scenarios": scenario_results,
                "success_rate": success_rate,
                "total_scenarios": len(scenarios)
            }
        )

        self.test_results.append(result)
        self._log_test_result(result)

    def _test_performance_characteristics(self):
        """Test performance characteristics"""

        latencies = []
        token_counts = []
        errors = 0

        # Run multiple quick requests
        for i in range(10):
            start_time = time.time()

            try:
                response = self.client.complete(f"Quick trading analysis #{i+1}: EURUSD at 1.09{i:02d}, RSI 6{i}, recommend action in one word.")

                latency = (time.time() - start_time) * 1000
                latencies.append(latency)
                token_counts.append(response.tokens_used)

                self.performance_data.append({
                    "test_id": i,
                    "latency_ms": latency,
                    "tokens": response.tokens_used,
                    "timestamp": datetime.now().isoformat()
                })

            except Exception as e:
                errors += 1
                latencies.append(10000)  # Penalty for errors

        # Calculate metrics
        avg_latency = sum(latencies) / len(latencies)
        p95_latency = sorted(latencies)[int(0.95 * len(latencies))]
        avg_tokens = sum(token_counts) / len(token_counts) if token_counts else 0
        error_rate = errors / 10

        # Performance targets
        latency_ok = avg_latency < 3000  # < 3 seconds average
        error_rate_ok = error_rate < 0.1  # < 10% error rate

        result = TestResult(
            test_name="performance_characteristics",
            success=latency_ok and error_rate_ok,
            duration_ms=sum(latencies),
            details={
                "avg_latency_ms": avg_latency,
                "p95_latency_ms": p95_latency,
                "avg_tokens": avg_tokens,
                "error_rate": error_rate,
                "targets_met": {
                    "latency": latency_ok,
                    "error_rate": error_rate_ok
                }
            }
        )

        self.test_results.append(result)
        self._log_test_result(result)

    def _test_concurrent_requests(self):
        """Test concurrent request handling"""

        async def make_request(request_id: int):
            try:
                start_time = time.time()
                response = self.client.complete(f"Concurrent test {request_id}: Quick EURUSD analysis")
                duration = (time.time() - start_time) * 1000
                return {"success": True, "duration": duration, "tokens": response.tokens_used}
            except Exception as e:
                return {"success": False, "error": str(e), "duration": 0}

        async def run_concurrent_test():
            # Run 5 concurrent requests
            tasks = [make_request(i) for i in range(5)]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            return results

        start_time = time.time()

        try:
            # Run the concurrent test
            concurrent_results = asyncio.run(run_concurrent_test())
            duration_ms = (time.time() - start_time) * 1000

            success_count = sum(1 for r in concurrent_results if isinstance(r, dict) and r.get("success"))
            success_rate = success_count / len(concurrent_results)

            result = TestResult(
                test_name="concurrent_requests",
                success=success_rate >= 0.8,  # 80% success rate
                duration_ms=duration_ms,
                details={
                    "concurrent_requests": len(concurrent_results),
                    "success_count": success_count,
                    "success_rate": success_rate,
                    "results": concurrent_results
                }
            )

        except Exception as e:
            result = TestResult(
                test_name="concurrent_requests",
                success=False,
                duration_ms=(time.time() - start_time) * 1000,
                details={},
                error=str(e)
            )

        self.test_results.append(result)
        self._log_test_result(result)

    def _test_error_handling(self):
        """Test error handling scenarios"""

        start_time = time.time()

        try:
            # Test with invalid API key
            bad_client = AnthropicLLMClient(api_key="invalid_key")

            try:
                response = bad_client.complete("Test message")
                # If this succeeds, something is wrong
                success = False
                error_handled = False
            except Exception:
                # Good - error was properly raised
                success = True
                error_handled = True

            result = TestResult(
                test_name="error_handling",
                success=success and error_handled,
                duration_ms=(time.time() - start_time) * 1000,
                details={
                    "error_properly_handled": error_handled,
                    "test_type": "invalid_api_key"
                }
            )

        except Exception as e:
            result = TestResult(
                test_name="error_handling",
                success=False,
                duration_ms=(time.time() - start_time) * 1000,
                details={},
                error=str(e)
            )

        self.test_results.append(result)
        self._log_test_result(result)

    def _test_fallback_scenarios(self):
        """Test fallback scenarios"""

        # This would test integration with MockLLMClient fallback
        # For now, just validate the concept

        result = TestResult(
            test_name="fallback_scenarios",
            success=True,  # Placeholder
            duration_ms=0,
            details={
                "note": "Fallback testing requires full integration setup",
                "recommendation": "Test manually with invalid API key"
            }
        )

        self.test_results.append(result)
        self._log_test_result(result)

    def _log_test_result(self, result: TestResult):
        """Log test result to console"""

        status = "✅ PASS" if result.success else "❌ FAIL"
        print(f"{status} {result.test_name} ({result.duration_ms:.1f}ms)")

        if result.error:
            print(f"    Error: {result.error}")
        elif result.details:
            # Print key details
            for key, value in result.details.items():
                if isinstance(value, (str, int, float, bool)):
                    print(f"    {key}: {value}")

    def _generate_test_report(self) -> dict[str, Any]:
        """Generate comprehensive test report"""

        total_tests = len(self.test_results)
        passed_tests = sum(1 for r in self.test_results if r.success)
        failed_tests = total_tests - passed_tests

        total_duration = sum(r.duration_ms for r in self.test_results)
        avg_duration = total_duration / total_tests if total_tests > 0 else 0

        # Performance analysis
        performance_metrics = None
        if self.performance_data:
            latencies = [p["latency_ms"] for p in self.performance_data]
            tokens = [p["tokens"] for p in self.performance_data]

            performance_metrics = PerformanceMetrics(
                avg_latency_ms=sum(latencies) / len(latencies),
                p95_latency_ms=sorted(latencies)[int(0.95 * len(latencies))],
                p99_latency_ms=sorted(latencies)[int(0.99 * len(latencies))],
                total_tokens=sum(tokens),
                avg_tokens_per_call=sum(tokens) / len(tokens),
                error_rate=0.0  # Will be calculated separately
            )

        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed": passed_tests,
                "failed": failed_tests,
                "success_rate": passed_tests / total_tests if total_tests > 0 else 0,
                "total_duration_ms": total_duration,
                "avg_duration_ms": avg_duration
            },
            "test_results": [asdict(r) for r in self.test_results],
            "performance_metrics": asdict(performance_metrics) if performance_metrics else None,
            "recommendations": self._generate_recommendations(),
            "timestamp": datetime.now().isoformat()
        }

        return report

    def _generate_recommendations(self) -> list[str]:
        """Generate recommendations based on test results"""

        recommendations = []

        # Check success rate
        success_rate = sum(1 for r in self.test_results if r.success) / len(self.test_results)

        if success_rate < 0.8:
            recommendations.append("❗ Low success rate - investigate failed tests before production deployment")

        # Check performance
        perf_test = next((r for r in self.test_results if r.test_name == "performance_characteristics"), None)
        if perf_test and perf_test.success:
            avg_latency = perf_test.details.get("avg_latency_ms", 0)
            if avg_latency > 2000:
                recommendations.append("⚠️ High latency detected - consider performance optimization")

        # Check trading decisions
        trading_test = next((r for r in self.test_results if r.test_name == "trading_decision"), None)
        if trading_test and not trading_test.success:
            recommendations.append("❗ Trading decision test failed - validate JSON response format")

        if not recommendations:
            recommendations.append("✅ All tests look good - ready for production deployment!")

        return recommendations

def main():
    """Run the test suite"""

    print("🧪 LLM Integration Test Suite")
    print("=" * 50)

    # Check for API key
    api_key = os.getenv("ANTHROPIC_API_KEY")
    if not api_key:
        print("❌ ANTHROPIC_API_KEY environment variable not set")
        print("Please set your API key and run again:")
        print("export ANTHROPIC_API_KEY='your_key_here'")
        return

    # Run tests
    tester = LLMIntegrationTester(api_key)
    report = tester.run_all_tests()

    # Print summary
    print("\n📊 TEST SUMMARY")
    print("=" * 30)
    summary = report["test_summary"]
    print(f"Total Tests: {summary['total_tests']}")
    print(f"Passed: {summary['passed']}")
    print(f"Failed: {summary['failed']}")
    print(f"Success Rate: {summary['success_rate']:.1%}")
    print(f"Total Duration: {summary['total_duration_ms']:.1f}ms")

    # Print recommendations
    print("\n💡 RECOMMENDATIONS")
    print("=" * 30)
    for rec in report["recommendations"]:
        print(f"  {rec}")

    # Save detailed report
    report_file = f"llm_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, "w") as f:
        json.dump(report, f, indent=2)

    print(f"\n📄 Detailed report saved to: {report_file}")

if __name__ == "__main__":
    main()
